{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d0d55f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.selection import SmartCorrelatedSelection, DropConstantFeatures, SelectByShuffling\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db8f2ac",
   "metadata": {},
   "source": [
    "## LECTURA DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfccfa1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\miniconda3\\envs\\Tuya\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import optuna\n",
    "\n",
    "def clean_value(x):\n",
    "    if isinstance(x, str):\n",
    "        x = x.replace('%', '').replace('$', '').replace(',', '')\n",
    "        x = x.replace(' ', '')\n",
    "    try:\n",
    "        return float(x)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "# Ruta del archivo\n",
    "ruta = r\"C:\\Users\\ASUS\\Documents\\GitHub\\tuya\\BASE_DS.xlsx\"\n",
    "\n",
    "# Cargar BASE\n",
    "base = pd.read_excel(ruta, sheet_name=\"BASE\")\n",
    "base = base[[\"Fecha\", \"facturacion\", \"saldo\"]]\n",
    "\n",
    "# Cargar MACROS\n",
    "macros = pd.read_excel(ruta, sheet_name=\"MACROS\")\n",
    "# macros.columns = macros.columns.str.strip()\n",
    "macros.columns = (\n",
    "    macros.columns\n",
    "    .str.replace('\\n', ' ', regex=True)  # Reemplaza saltos de línea por espacio\n",
    "    .str.replace('\\s+', ' ', regex=True) # Colapsa espacios múltiples en uno solo\n",
    "    .str.strip()                          # Quita espacios al inicio y fin\n",
    ")\n",
    "# Apply cleaning to all except the first column (Fecha)\n",
    "for col in macros.columns[1:]:\n",
    "    macros[col] = macros[col].apply(clean_value)\n",
    "\n",
    "# Convertir fecha \n",
    "macros['Fecha'] = pd.to_datetime(macros['Fecha'], format=\"%b-%y\", errors='coerce')\n",
    "\n",
    "\n",
    "# Convertir fecha \n",
    "base['Fecha'] = pd.to_datetime(base['Fecha'], format=\"%b-%y\", errors='coerce')\n",
    "base.set_index('Fecha', inplace=True)\n",
    "\n",
    "# ===============================\n",
    "# 1. Cargar datos\n",
    "# ===============================\n",
    "df = pd.merge(macros, base, left_on='Fecha', right_index=True, how='inner')\n",
    "df = df.set_index('Fecha')\n",
    "df = df.sort_index()\n",
    "\n",
    "# ===============================\n",
    "# 2. Crear variables target como cambio porcentual (0-1)\n",
    "# ===============================\n",
    "df['facturacion_change'] = df['facturacion'].pct_change(fill_method=None)\n",
    "df['saldo_change'] = df['saldo'].pct_change(fill_method=None)\n",
    "\n",
    "# ===============================\n",
    "# 3. Features y targets\n",
    "# ===============================\n",
    "feature_cols = [col for col in df.columns if col not in ['facturacion','saldo','facturacion_change','saldo_change']]\n",
    "target_fact = ['facturacion_change']\n",
    "target_sald = ['saldo_change']\n",
    "\n",
    "escenario_keywords = ['pesimista', 'alterno', 'optimista']\n",
    "feature_cols = [\n",
    "    col for col in feature_cols \n",
    "    if not any(keyword.lower() in col.lower() for keyword in escenario_keywords)\n",
    "]\n",
    "\n",
    "data_fact = df[feature_cols + target_fact].dropna(subset=feature_cols).dropna(how='any')\n",
    "data_sald = df[feature_cols + target_sald].dropna(subset=feature_cols).dropna(how='any')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df702006",
   "metadata": {},
   "source": [
    "### Estrategia utilizada\n",
    "El procedimiento seguido se centró en:\n",
    "\n",
    "1. **Evaluación de la multicolinealidad:**  \n",
    "   Se analizaron las correlaciones entre variables para detectar redundancias.  \n",
    "   Cuando dos o más variables mostraron una correlación muy alta, se consideró que aportaban información similar.\n",
    "\n",
    "2. **Selección basada en información mutua:**  \n",
    "   Para cada grupo de variables altamente correlacionadas, se utilizó la **información mutua** con la variable objetivo como criterio de selección.  \n",
    "   La información mutua mide la dependencia entre dos variables y nos indica cuánto conocimiento adicional sobre la variable objetivo aporta una variable específica.  \n",
    "   De este modo, se conservó la variable que maximiza la información mutua con el target, garantizando mayor relevancia predictiva."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dfe1d9",
   "metadata": {},
   "source": [
    "## FUNCIONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2317b73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_correlated_features(df, numvars, target, corr_limit=0.85, selection_method=\"variance\"):\n",
    "    \"\"\"   \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The dataframe containing the dataset from which features will be selected and reduced.\n",
    "    numvars (list): A list of numeric variable names (columns in the dataframe) that are to be considered for the feature selection process.\n",
    "    target (str or list): The name of the target variable column in the dataframe. If a list is provided, the first element is considered as the target variable.\n",
    "    corr_limit (float, optional): The correlation threshold used to identify highly correlated pairs of variables. Default is 0.90.\n",
    "    selection_method (str): Takes the values “missing_values”, “cardinality”, “variance” and \"mutual_information_score\".\n",
    "        “missing_values”: keeps the feature from the correlated group with the least missing observations. Developed by feature_engine library.\n",
    "        “cardinality”: keeps the feature from the correlated group with the highest cardinality. Developed by feature_engine library.\n",
    "        “variance”: keeps the feature from the correlated group with the highest variance. Developed by feature_engine library.\n",
    "        \"mutual_information_score\": keeps the feature from the correlated group that has the highest mutual information score with the target. Developed by featurewiz library.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    list: A list of selected features that are uncorrelated and have high mutual information scores with the target.\n",
    "    \"\"\"\n",
    "    def remove_correlated_features_simple(df, numvars, threshold, selection_method):\n",
    "        \"\"\"\n",
    "        Removes correlated features using smartcorrelatedselection from feature_engine\n",
    "        \"\"\"\n",
    "        scs = SmartCorrelatedSelection(threshold=threshold, selection_method=selection_method)\n",
    "        no_correlated_features = scs.fit_transform(df[numvars])\n",
    "        no_correlated_features_ = no_correlated_features.columns\n",
    "        return no_correlated_features_\n",
    "\n",
    "    def find_remove_duplicates(list_of_values):\n",
    "        \"\"\"\n",
    "        Removes duplicates from a list to return unique values\n",
    "        \"\"\"\n",
    "        output = []\n",
    "        seen = set()\n",
    "        for value in list_of_values:\n",
    "            if value not in seen:\n",
    "                output.append(value)\n",
    "                seen.add(value)\n",
    "        return output\n",
    "    \n",
    "    def return_dictionary_list(lst_of_tuples):\n",
    "        \"\"\" \n",
    "        Returns a dictionary of lists if you send in a list of Tuples\n",
    "        \"\"\"\n",
    "        orDict = defaultdict(list)\n",
    "        # iterating over list of tuples\n",
    "        for key, val in lst_of_tuples:\n",
    "            orDict[key].append(val)\n",
    "        return orDict\n",
    "\n",
    "    def left_subtract(l1, l2):\n",
    "        \"\"\"\n",
    "        Returns a list of elements in l1 that are not in l2\n",
    "        \"\"\"\n",
    "        lst = []\n",
    "        for i in l1:\n",
    "            if i not in l2:\n",
    "                lst.append(i)\n",
    "        return lst\n",
    "    \n",
    "    # Deep copy of the dataframe to avoid modifying the original one\n",
    "    df = deepcopy(df)\n",
    "    # Extract the target column\n",
    "    print('#COLUMNS')\n",
    "    print(df.columns)\n",
    "    df_target = df[target]\n",
    "\n",
    "    # Deep copy of the target to avoid modifying the original one\n",
    "    target = deepcopy(target)\n",
    "\n",
    "    if selection_method == \"mutual_information_score\":\n",
    "\n",
    "        # Calculate the absolute correlation matrix, sort it, and remove duplicates\n",
    "        correlation_dataframe = df.corr().abs().unstack().sort_values().round(7).drop_duplicates()\n",
    "\n",
    "        # Convert the correlation data to a dataframe\n",
    "        corrdf = pd.DataFrame(correlation_dataframe[:].reset_index())\n",
    "        corrdf.columns = ['var1', 'var2', 'coeff']\n",
    "\n",
    "        # Filter the correlation dataframe to get pairs with correlation above the threshold\n",
    "        corrdf1 = corrdf[corrdf['coeff'] >= corr_limit]\n",
    "\n",
    "        # Remove self-correlations\n",
    "        corrdf1 = corrdf1[corrdf1['var1'] != corrdf1['var2']]\n",
    "        \n",
    "        # Create a list of correlated pairs\n",
    "        correlated_pair = list(zip(corrdf1['var1'].values.tolist(), corrdf1['var2'].values.tolist()))\n",
    "        \n",
    "        # Convert the list of correlated pairs to a dictionary\n",
    "        corr_pair_dict = dict(return_dictionary_list(correlated_pair))\n",
    "        \n",
    "        # Get a list of unique variables that are part of correlated pairs\n",
    "        corr_list = find_remove_duplicates(corrdf1['var1'].values.tolist() + corrdf1['var2'].values.tolist())\n",
    "        \n",
    "        # Get keys from the correlation dictionary\n",
    "        keys_in_dict = list(corr_pair_dict.keys())\n",
    "        \n",
    "        # Create a reverse mapping of correlated pairs\n",
    "        reverse_correlated_pair = [(y, x) for (x, y) in correlated_pair]\n",
    "        reverse_corr_pair_dict = dict(return_dictionary_list(reverse_correlated_pair))\n",
    "        \n",
    "        # Merge the original and reverse correlation dictionaries\n",
    "        for key, val in reverse_corr_pair_dict.items():\n",
    "            if key in keys_in_dict:\n",
    "                if len(key) > 1:\n",
    "                    corr_pair_dict[key] += val\n",
    "            else:\n",
    "                corr_pair_dict[key] = val\n",
    "        \n",
    "        # Check if there are no correlated variables\n",
    "        if len(corr_list) == 0:\n",
    "            final_list = list(correlation_dataframe)\n",
    "            print('    Selecting all (%d) variables since none of numeric vars are highly correlated...' % len(numvars))\n",
    "            return numvars\n",
    "        \n",
    "        # Handle target variable if it is provided as a list\n",
    "        if isinstance(target, list):\n",
    "            target = target[0]\n",
    "        \n",
    "        max_feats = len(corr_list)\n",
    "        sel_function = mutual_info_regression\n",
    "\n",
    "        # Ensure there are no infinite or null values in the dataframe\n",
    "        df_fit = df[corr_list]\n",
    "        \n",
    "        # Drop rows with NaN values if present\n",
    "        if df_fit.isnull().sum().sum() > 0:\n",
    "            df_fit = df_fit.dropna()\n",
    "        else:\n",
    "            print('    there are no null values in dataset...')\n",
    "        \n",
    "        # Check for null values in the target column\n",
    "        if df_target.isnull().sum().sum() > 0:\n",
    "            print('    there are null values in target. Returning with all vars...')\n",
    "            return numvars\n",
    "        else:\n",
    "            print('    there are no null values in target column...')\n",
    "        \n",
    "        try:\n",
    "            # Calculate mutual information scores\n",
    "            fs = mutual_info_regression(df_fit, df_target, n_neighbors=5, discrete_features=False, random_state=42)\n",
    "        except:\n",
    "            print('    function is erroring. Returning with all %s variables...' % len(numvars))\n",
    "            return numvars\n",
    "        \n",
    "        try:\n",
    "            #################################################################################\n",
    "            #######   This is the main section where we use mutual info score to select vars        \n",
    "            #################################################################################\n",
    "            mutual_info = dict(zip(corr_list, fs))\n",
    "            \n",
    "            # Sort variables by mutual information score in descending order\n",
    "            sorted_by_mutual_info = [key for (key, val) in sorted(mutual_info.items(), key=lambda kv: kv[1], reverse=True)]\n",
    "            \n",
    "            selected_corr_list = []\n",
    "            \n",
    "            # Make multiple copies of the sorted list since it is iterated many times\n",
    "            orig_sorted = deepcopy(sorted_by_mutual_info)\n",
    "            copy_sorted = deepcopy(sorted_by_mutual_info)\n",
    "            copy_pair = deepcopy(corr_pair_dict)\n",
    "            \n",
    "            # Select each variable by the highest mutual info and see what vars are correlated to it\n",
    "            for each_corr_name in copy_sorted:\n",
    "                selected_corr_list.append(each_corr_name)\n",
    "                for each_remove in copy_pair[each_corr_name]:\n",
    "                    if each_remove in copy_sorted:\n",
    "                        copy_sorted.remove(each_remove)\n",
    "            \n",
    "            # Combine the uncorrelated list to the selected correlated list above\n",
    "            rem_col_list = left_subtract(numvars, corr_list)\n",
    "            final_list = rem_col_list + selected_corr_list\n",
    "            removed_cols = left_subtract(numvars, final_list)\n",
    "        except Exception as e:\n",
    "            print('    SULOV Method crashing due to %s' % e)\n",
    "            removed_cols = remove_highly_correlated_vars_fast(df, corr_limit)\n",
    "            final_list = left_subtract(numvars, removed_cols)\n",
    "        print(f'Completed using {selection_method}. Features selected: {len(final_list)}')\n",
    "\n",
    "    elif selection_method in [\"missing_values\", \"cardinality\", \"variance\", \"model_performance\"]:    \n",
    "        final_list = list(remove_correlated_features_simple(df, numvars, threshold=corr_limit, selection_method=selection_method))\n",
    "        print(f'Completed using {selection_method}. Features selected: {len(final_list)}')\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca9752c",
   "metadata": {},
   "source": [
    "### FEATURES SELECCIONADAS PARA FACTURACION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "975a74dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#COLUMNS\n",
      "Index(['PIB (var. % anual, nominal)', 'CH2', 'Desempleo', 'Inflacion', 'DTF',\n",
      "       'IBR', 'Tasa_Cambio', 'TASA REPO', 'facturacion_change'],\n",
      "      dtype='object')\n",
      "    there are no null values in dataset...\n",
      "    there are no null values in target column...\n",
      "Completed using mutual_information_score. Features selected: 4\n"
     ]
    }
   ],
   "source": [
    "res_fact = remove_correlated_features(data_fact, feature_cols, target_fact[0], corr_limit=0.8, selection_method=\"mutual_information_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e6dac6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Desempleo', 'Tasa_Cambio', 'TASA REPO', 'PIB (var. % anual, nominal)']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_fact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1803926",
   "metadata": {},
   "source": [
    "### FEATURES SELECCIONADAS PARA SALDO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a925e319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#COLUMNS\n",
      "Index(['PIB (var. % anual, nominal)', 'CH2', 'Desempleo', 'Inflacion', 'DTF',\n",
      "       'IBR', 'Tasa_Cambio', 'TASA REPO', 'saldo_change'],\n",
      "      dtype='object')\n",
      "    there are no null values in dataset...\n",
      "    there are no null values in target column...\n",
      "Completed using mutual_information_score. Features selected: 4\n"
     ]
    }
   ],
   "source": [
    "res_sald = remove_correlated_features(data_sald, feature_cols, target_sald[0], corr_limit=0.8, selection_method=\"mutual_information_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8e68d48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Desempleo', 'Tasa_Cambio', 'TASA REPO', 'CH2']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_sald"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520e9aa3",
   "metadata": {},
   "source": [
    "### Resultados del proceso\n",
    "Tras aplicar este método, se definieron los conjuntos óptimos de variables para cada cuenta a pronosticar:\n",
    "\n",
    "- **Para Facturación:**  \n",
    "  - `Desempleo`  \n",
    "  - `Tasa_Cambio`  \n",
    "  - `TASA REPO`  \n",
    "  - `PIB (var. % anual, nominal)`\n",
    "\n",
    "- **Para Saldo:**  \n",
    "  - `Desempleo`  \n",
    "  - `Tasa_Cambio`  \n",
    "  - `TASA REPO`  \n",
    "  - `CH2`\n",
    "\n",
    "---\n",
    "\n",
    "### Beneficios obtenidos\n",
    "Este proceso permitió:\n",
    "- Reducir la **complejidad del modelo** al trabajar solo con las variables más relevantes.\n",
    "- Disminuir el riesgo de **sobreajuste** (overfitting).\n",
    "- Mejorar la **capacidad de generalización** de los modelos y aumentar la precisión de los pronósticos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tuya",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
